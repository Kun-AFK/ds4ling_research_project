---
title             : "Assessing the Impact of Physicochemical Properties on Red Wine Quality: A Multivariate (OLS and Lasso) Regression and Bayesian Approach"

author: 
  - name          : "Jukun Zhang"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    email         : "jz1250@scarletmail.rutgers.edu"

affiliation:
  - id            : "1"
    institution   : "Rutgers University"


abstract: |
  This study mainly uses three models-OLS, Lasso regression and Bayes model. After preliminary exploratory analysis (descriptive statistics, box plots, correlation heat maps, paired scatter plots), OLS can explain 36% of the variance (adjusted R² = 0.356), thus determining that alcohol, volatile acid, sulfate, chloride, pH, free sulfur dioxide and total sulfur dioxide are important predictors. Lasso regression reduces 3 variables by setting the L1 penalty coefficient. Although the model is further optimized, the results are basically the same as OLS. However, the Bayes model extracts more accurate prediction variables through 95% confidence interval and KDE test, and believes that the positive impact of wine quality comes from free SO₂ (FSO2), sulfates (Sulphates) and alcohol (Alcohol), and the negative impact comes from volatile acid (VolAcid), chloride (Chlor), and total SO₂ (TSO2). Finally, the data_wine_tidy data is binarized, and Logistic regression is used to verify that the prediction accuracy of the final confirmed 6 variables for the quality of red wine is 0.818, indicating that these six variables can provide a certain quantitative basis for the prediction of red wine quality.
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "OLS; Lasso Regression; Bayes Regression; Logistics Regression"
wordcount         : "X"

bibliography      : "r-references.bib"

floatsintext      : yes
linenumbers       : no
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "doc"
output            : 
  papaja::apa6_pdf :
    latex_engine: xelatex
    fig_caption: yes


header-includes:
  - \usepackage{float}
  - \usepackage[justification=centering]{caption} 
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```


\newpage
# 2 Introduction & Literature Review
The quality of red wine not only affects consumers' tasting experience and health perception, but is also directly related to the improvement direction of grape planting and brewing technology and the optimization of the industrial value chain. With the continuous advancement of analytical instruments and data acquisition technology, researchers can obtain a series of physical and chemical indicators including fixed acidity, volatile acidity, citric acid content, residual sugar, chloride, free sulfur dioxide, total sulfur dioxide, density, pH value, sulfate and alcohol content. These indicators have their own characteristics in different regions, different years and different brewing process conditions, and have a complex and multidimensional impact on the sensory score (quality) of the final wine.

Traditional research mainly uses ordinary least squares (OLS) regression to evaluate the linear relationship between various physical and chemical properties and scores. Some scholars combine ridge regression or principal component analysis to deal with multicollinearity problems. In recent years, Lasso regression has been widely used in high-dimensional data modeling because of its variable selection and regularization capabilities; while Bayesian regression provides a framework for uncertainty quantification for parameter estimation through prior distribution and posterior inference. However, there are few existing literatures that systematically compare the model performance and variable importance differences of OLS, Lasso and Bayesian methods on the same data set.

Based on the red wine quality data set (n = 1,599) provided by UCI, this study uses OLS, Lasso and Bayesian regression as analysis tools. Through descriptive statistics, visual exploration and cross-validation, this study comprehensively evaluates the relative impact of various physical and chemical properties on wine quality scores, and compares the advantages and disadvantages of the three methods in terms of prediction accuracy and variable explanatory power, providing a quantitative basis for red wine quality control and optimization. <!-- 21-word solution (Simmons, Nelson & Simonsohn, 2012; retrieved from http://ssrn.com/abstract=2160588) -->

\newpage
# 3 Data and Exploratory Analysis
## 3.1 Data Source & Pre-processing
My data source was find from "UCI Red Wine Quality" [link](https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv). It has 1599 data points and I choose "Quality" as the response and other 11 variables as predictors which include fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chloride, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulfate and alcohol.
After preprocessing the data, I first read the raw data as *data_wine_raw*, and then checked whether there were missing values in the raw data. I used "anyNA(data_wine_raw)" and "colSums(is.na(data_wine_raw))" to test whether there were missing values and which column of variables had missing values. The test results showed that the data was very intact and there were no missing values. The next step was to continue to abbreviate the column names to make it easier to represent in the output image and make the results easier to read.
Part of the data_wine_tidy are show here:

```{r show-mini-table, echo=FALSE, message=FALSE, warning=FALSE}
# Loading necessary library
library(tidyverse)
library(here)
library(dplyr)
library(tidyr)
library(ds4ling)
library(lme4)
library(lmerTest)
library(GGally)       # ggpairs()
library(corrplot)     # corrplot()
library(ggcorrplot)   # ggcorrplot()
library(glmnet)
library(caret)
library(rstanarm)
library(knitr)
library(brms)
library(kableExtra)
library(tibble)

# Read the raw data
data_wine_raw <- read.csv(
  here("data_raw", "winequality-red.csv"),
  header = TRUE,
  sep = ";",
  stringsAsFactors = FALSE
)

data_wine_tidy <- data_wine_raw |>
  rename(
    FixAcid = fixed.acidity,
    VolAcid = volatile.acidity,
    CitAcid = citric.acid,
    Sugar   = residual.sugar,
    Chlor   = chlorides,
    FSO2    = free.sulfur.dioxide,
    TSO2    = total.sulfur.dioxide,
    Dens    = density,
    pH      = pH,
    Sulph   = sulphates,
    Alc     = alcohol,
    Qual    = quality
  )

mini_table <- data_wine_tidy |>
  select(FixAcid, VolAcid, Sugar, Chlor, Alc, Qual) |>
  head(6)
knitr::kable(mini_table,
      caption = "data\\_wine\\_tidy (Part vars)",
      align   = "c",
      escape  = FALSE
) |>
  kableExtra::kable_styling(position = "center",latex_options  = c("hold_position"))
```

## 3.2 Descriptive Statistics
Then I exported Five-Number Summaries to further analyze the data.

The Five-Number Summaries (Table 2) highlight the central tendency and variance of each physicochemical variable in the red wine dataset. For most acidity measurements (fixed acidity, volatile acidity, citric acid), the interquartile ranges (IQRs) were moderate, about 2.1 units for fixed acidity and 0.25 units for volatile acidity, indicating a fairly consistent distribution of acidity across samples, although the maximum values (15.9 for fixed acidity and 1.58 for volatile acidity) indicate a right skew and a high acidity outlier. Sugar content was clearly positively skewed. The median sugar content was only 2.2 g/L, but the maximum reached 15.5 g/L, reflecting the small number of very sweet wines. Chloride and pH show narrow IQRs (0.02 g/L and 0.19 pH units, respectively), indicating strict quality control over salinity and acidity balance. Density was fairly constant (only 0.996-0.998 from Q1-Q3), confirming little variation in ethanol to water ratio. Sulfate (IQR = 0.18 g/L) and alcohol (IQR = 1.6 % v/v) both show moderate distributions, with the upper quartile (11.1 %) and maximum value (14.9 %) of alcohol highlighting that wines with higher alcohol content may have higher quality scores. Finally, total SO₂ and free SO₂ showed the largest absolute ranges (TSO₂ up to 289 mg/L, FSO₂ up to 72 mg/L), indicating different SO₂ conservation strategies, while the mass itself ranged from 3 to 8, but was concentrated at 5 to 6 (IQR = 1). Combining these summaries allows us to predict which predictors contribute to quality changes and which nonlinear effects or outliers require further investigation.

```{r five-num, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
five_num_summary <- data_wine_tidy |>
  summarise(across(
    .cols = everything(),
    .fns = list(
      Min    = ~ min(.x, na.rm = TRUE),
      Q1     = ~ quantile(.x, 0.25, na.rm = TRUE),
      Median = ~ median(.x, na.rm = TRUE),
      Q3     = ~ quantile(.x, 0.75, na.rm = TRUE),
      Max    = ~ max(.x, na.rm = TRUE)
    ),
    .names = "{.col}_{.fn}"
  )) |>
  pivot_longer(everything(),
               names_to   = c("variable", "stat"),
               names_sep  = "_",
               values_to  = "value") |>
  pivot_wider(names_from = stat, values_from = value)

knitr::kable(
  five_num_summary,
  caption = "Table 2. Five-Number Summaries for All Variables",
  digits  = 2,
  align   = "c"
) |>
  kable_styling(
    position      = "center",
    latex_options = "hold_position"
  )
```

## 3.3 Visualization & Correlation
After completing the above mean and median analysis, I output boxplots, heat maps, and Pairwise Scatterplot Matrix of some variables and wine quality, hoping to use these visualizations to analyze the correlation between each variable and response (quality) and check whether there is multicollinearity. The output images are as follows:

```{r fig-box, echo=FALSE, out.width='50%', fig.align='center',fig.pos='H',fig.cap="Boxplot of part Variables"}
knitr::include_graphics(here::here("plots", "bp1.png"))
```

```{r heat, echo=FALSE, out.width='50%', fig.align='center',fig.pos='H',fig.cap="Heatmap of correlations between variables"}
knitr::include_graphics(here::here("plots","r1.png"))
```

Figure 2 shows the Pearson correlation coefficient matrix of all eleven variables and the dependent variable wine quality. From the figure, we can see that alcohol and taste show a strong positive correlation (r ≈ +0.48), which indicates that wines with higher alcohol content tend to get higher quality scores. On the other hand, volatile acidity is significantly negatively correlated with health (r ≈ -0.39), indicating that the higher the volatile acidity, the lower the quality score of the wine. We also found that there may be some multicollinearity between the predictors - free SO2 and total SO2 (r = +0.67) and fixed acidity and density (r = +0.67). In short, based on this figure, we have a deeper understanding of the data, and the conclusions we get will help guide the subsequent OLS and Lasso regression methods.

```{r Pairwise, echo=FALSE, out.width='50%', fig.align='center',fig.pos='H',fig.cap="Pairwise Scatterplot Matrix for all variables"}
knitr::include_graphics(here::here("plots","p1.png"))
```
Figure 3 uses GGally::ggpairs to output the Pairwise Scatterplot Matrix, where the lower left corner of the diagonal outputs the scatter plots between the variables. From the scatter plots of the variables, we can see that the points of Fixed Acidity and Density are dense and distributed in an upward diagonal, reflecting the conclusion drawn from the heat map that the two variables may be highly collinear. The output in the middle of the diagonal is the kernel density estimates of each variable’s marginal distribution, and the output in the upper right corner of the diagonal is the same Pearson correlation coefficient as the heat map. The smooth density curves of each variable reveal the distribution characteristics: for example, Sugar shows an obvious right-skewed long-tail distribution, while Density is highly concentrated in the range of 0.997–0.998. This matrix not only further confirms the known strong associations such as Sulphates and Quality, but also once again shows the multicollinearity problem and the abnormal marginal distribution of some variables, providing a key basis for variable screening and diagnosis of subsequent regression models.

\newpage
# 4 Methodology & Results
## 4.1 Ordinary Least Squares Model (OLS)
I first used the OLS model to linearly model the dependent variable wine quality for all 11 variables. The regression equation is as follows:
\begin{equation}
\label{eq:ols}
\mathrm{Qual}_i
= \beta_0
+ \beta_1\,\mathrm{FixAcid}_i
+ \beta_2\,\mathrm{VolAcid}_i
+ \cdots
+ \beta_{11}\,\mathrm{Alc}_i
+ \varepsilon_i,
\quad
\varepsilon_i \sim N(0,\sigma^2).
\end{equation}

Hypothesis tests are performed on the parameters to test the significance of the parameters. The results can be obtained from the P-value column in Table 3. For each regression coefficient $\beta_j$, test:

Null Hypothesis($H_0$) : $\beta_j$ = 0 vs Alternative Hypothesis($H_a$) : at least one $\beta_j$ $≠$ 0

Calculate the $t$ statistic $t_j = \hat\beta_j / \mathrm{SE}(\hat\beta_j)$ and report the two-tailed $p$ value. The results show:

```{r ols-model, echo=FALSE, results='asis'}
model_ols <- lm(Qual ~ ., data = data_wine_tidy)
apa_table(
  summary(model_ols)$coef,
  caption = "OLS regression coefficient estimation and significance test",
  label   = "tab:ols",
  placement = "H"
)
```
Among them, we can find that the parameters of volatile acidity, chloride, total $SO_2$, free $SO_2$, pH, sulphates and alcohol are significant in the model, while other variables are not significant at the 5% level. We can preliminarily conclude that these variables shown as significant above have a certain effect on the quality score of red wine. The fitting results of the OLS model show that the coefficient of determination $R^2 = 0.3606$ indicates that the model can explain about 36.1% of the total variance of the quality score. However, after adjustment, $R^2$ = 0.3561, and the F statistic of the model = 81.35 (df = 11, 1587), $p<2.2\times10^{-16}$, rejecting the null hypothesis, indicating that the model is significant.

After that, the OLS model residuals were further tested using the diagnosis() function in the library (ds4ling). The following is the output of the OLS residual model diagnostic graph:

```{r fig-ols-res, echo=FALSE, out.width='50%', fig.align='center', fig.pos='H',fig.cap="Residual diagnosis for OLS model"}
knitr::include_graphics(here::here("plots","OLS_res.png"))
```
The residuals vs. fitted values plot on the left of the test plot shows randomly distributed scatter with no particularly obvious pattern, so the assumption of linearity can be considered supported. However, since the variance seems to increase with the fitted values, there may be slight heteroskedasticity. The Q-Q plot on the right shows that the residuals are roughly distributed along the diagonal (r = 0.95), indicating approximate normality. However, the slight deviation in the tails shows that there may be slight non-normality. The residual density plot in the middle is centered at zero and has a single peak, which is similar to the residual density plot of a good model fit, and is bell-shaped, but a slight skewness is observed.


## 4.2 Lasso Regression
To mitigate multicollinearity and enforce sparsity in our predictor set, we fit a Lasso model using the **glmnet** package.  In matrix notation, letting \(x_i\) denote the \(p\)-dimensional predictor vector for observation \(i\) and \(y_i\) its quality score, the Lasso estimate solves

\begin{equation}
\label{eq:lasso}
\min_{\beta_0,\beta}
  \frac{1}{N}\sum_{i=1}^N \bigl(y_i - \beta_0 - x_i^\top \beta\bigr)^2
  \;+\;
  \lambda \sum_{j=1}^p \lvert \beta_j\rvert,
\quad
\lambda \ge 0.
\end{equation}

Prior to fitting, all predictors were standardized and an intercept term retained.
### λ Selection via 10-Fold Cross-Validation
Using the glmnet() function, with the penalty parameter $α$ set to 1, we can get a complete Lasso model. We continue to set the response to the quality of the wine, and the dependent variable is the remaining 11 variables. We hope to use the Lasso model to delete as many unimportant variables as possible, making the model parameters simpler - that is, to get a sparse model. This way, we can more clearly understand which physical and chemical properties (variables) have a more significant impact on the quality of the wine.We then output a plot of $Log(λ)$ and Mean Square Error to help determine the optimal $λ$ value.
```{r lasso_model, echo=FALSE, message=FALSE, warning=FALSE}
# Prepare matrices: x (independent variables), y (dependent variable)
x <- model.matrix(Qual ~ ., data = data_wine_tidy)[, -1]
y <- data_wine_tidy$Qual

# 7.1 10-fold cross-validation to find optimal λ
set.seed(2025)
cv_lasso <- cv.glmnet(x, y, alpha = 1, family = "gaussian", standardize = TRUE, nfolds = 10)
best_lambda <- cv_lasso$lambda.min
lasso_coef <- coef(cv_lasso, s = "lambda.min")
```

```{r fig-cv-lasso, echo=FALSE, out.width='50%', fig.align='center', fig.pos='H',fig.cap="Plot of CV\\_Lasso"}  
knitr::include_graphics(here::here("plots","cv_lasso.png"))  
```
The figure shows that as $Log(λ)$ increases from -7 to -1, the MSE of the model also increases. To prevent overfitting, we believe that $Log(λ)$ = -3 is the best value, because reducing $Log(λ)$ further will penalize the model too much and delete some necessary parameters. The number at the top (11 to 0) indicates the number of factors with non-zero coefficients. As $λ$ increases, fewer predictors are selected, thereby reducing the complexity of the model. At this optimal $λ$, Lasso retains the following 8 non-zero coefficients.

Compute the cross-validation MSE for a sequence of $λ$ values. The value (denoted as $λ_{min}$) that minimizes the CV error is found to be.
$\hatλ_{min}$ = 0.00848

```{r lasso-coef-table, echo=FALSE, results='asis'}
coef_df <- as.matrix(lasso_coef) %>%
  as.data.frame() %>%
  tibble::rownames_to_column("term") %>%
  rename(estimate = `s1`) %>%
  filter(estimate != 0)

knitr::kable(
  coef_df,
  caption = "Non-zero Lasso coefficients at \\(\\hat{\\lambda}_{\\min}\\)",
  digits  = 4,
  align   = "l",
  escape  = FALSE
) %>%
  kable_styling(
    position      = "center",
    latex_options = "hold_position"
  )
```
\newpage
Three predictors：fixed acidity, citric acid, and density are driven exactly to zero, indicating minimal marginal contribution once penalization is applied. The retained coefficients largely mirror OLS signs and magnitudes, reaffirming volatile acidity and chlorides as the negative drivers of quality and sulphates and alcohol content as positive drivers.

Afterwards, I used 10-fold cross validation to compare the prediction performance of OLS and Lasso models.
```{r figcp, echo=FALSE,out.width='50%', fig.align='center', fig.pos='H', fig.cap="OLS and Lasso Model Performance Comparison"}
knitr::include_graphics(here::here("plots", "cp1.png"))
```
From the comparison of the 10-fold cross validation of the two models, it can be seen that LASSO and OLS perform similarly in predicting wine quality, with little difference in MAE, RMSE, and R².

## 5.3 Bayesian Cumulative Logit Regression

To account for the ordinal nature of our quality scores (3–8), we replace the Gaussian assumption with a Bayesian cumulative logit model, using the brms package. This framework estimates cutpoints (intercepts) that partition the latent propensity into the seven observed quality levels, and slopes for each predictor, with full posterior uncertainty.
**Model specification**

\[
\Pr\bigl(\text{Qual}_i \le k \mid x_i\bigr)
=\mathrm{logit}^{-1}\bigl(\gamma_k - x_i^\top\beta\bigr),
\quad k = 1,\dots,7,
\]
where \(\gamma_1<\gamma_2<\cdots<\gamma_7\) are the cutpoints and \(\beta\) the vector of slopes.  Priors:

\[
\beta_j \sim \mathrm{Normal}(0,1),\quad 
\gamma_k \sim \mathrm{Normal}(0,5).
\]

The model was fit with four NUTS chains, 2,000 iterations each (1,000 warmup), yielding 4,000 post–warmup draws.

**Posterior summary & convergence**  
Table X reports posterior medians, standard deviations, and 95% credible intervals for each slope.  Convergence diagnostics are excellent (\(\widehat R = 1.00\), Bulk ESS & Tail ESS >1,500), though 83 divergent transitions suggest increasing `adapt_delta` in future work.

```{r fit-bayes, echo=FALSE, include=FALSE}
# Fit the Bayes model
priors <- c(
  set_prior("normal(0, 1)", class = "b"),
  set_prior("normal(0, 5)", class = "Intercept")
)

model_bayes <- brm(
  Qual ~ .,
  data            = data_wine_tidy,
  family          = cumulative(link = "logit"),
  prior           = priors,
  chains          = 4,
  cores           = parallel::detectCores(),
  iter            = 2000,
  seed            = 2025
)
```

```{r bayes-summary, echo=FALSE, results='asis'}

post_fixed <- summary(model_bayes)$fixed

apa_table(
  post_fixed,
  caption   = "Posterior medians, standard deviations, and 95\\% CIs for the Bayesian cumulative logit model",
  label     = "tab:bayes-summary",
  placement = "H"
)

```


Then do the Posterior predictive check for the Bayesian cumulative logit regression model. And use pp_check() to output the diagnosis figure:
```{r ppck, echo=FALSE, out.width='60%',  fig.align='center', fig.pos='H', fig.cap="Posterior predictive check: observed vs. replicated quality distributions"}
knitr::include_graphics(here::here("plots","kde.png"))
```

Figure 7 overlays the kernel density of the observed quality distribution ($y$, dark blue) against 10 replicated datasets ($y_{rep}$, light blue). The replicated densities closely track the empirical peaks at scores 5, 6 and 7, demonstrating that the model captures the overall shape and modal structure of the data, with only extreme slight under‐coverage of the low (3–4) tail and large(7-8) tail.

# 6 Conclusion and Future Directions
##
























## Participants

## Material

## Procedure

## Data analysis
We used `r cite_r("r-references.bib")` for all our analyses.


# Results

# Discussion


\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
